{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import time\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n"
     ]
    }
   ],
   "source": [
    "# Preparing for Data\n",
    "print('==> Preparing data..')\n",
    "#classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        self.convnet = nn.Sequential(OrderedDict([\n",
    "            ('c1',nn.Conv2d(1,6,kernel_size=(5,5))),\n",
    "            ('relu1',nn.ReLU()),\n",
    "            ('s2',nn.MaxPool2d(kernel_size=(2,2), stride=2)),\n",
    "            ('c3',nn.Conv2d(6,16,kernel_size=(5,5))),\n",
    "            ('relu3',nn.ReLU()),\n",
    "#             ('drop3',nn.Dropout2d(p=0.05)),\n",
    "            ('s4',nn.MaxPool2d(kernel_size=(2,2), stride=2))\n",
    "        ]))\n",
    "        \n",
    "        self.fc = nn.Sequential(OrderedDict([\n",
    "            ('f5',nn.Linear(256,120)),\n",
    "            ('relu5',nn.ReLU()),\n",
    "            ('f6',nn.Linear(120,84)),\n",
    "            ('relu6',nn.ReLU()),\n",
    "            ('f7',nn.Linear(84,10)),\n",
    "            ('sig7',nn.LogSoftmax(dim=-1))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # print(len(x[0]))\n",
    "        out = self.convnet(x)\n",
    "        # print(out.size(),len(out[0]))\n",
    "        # x=x.view(128,120)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    count = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output  = model(data)\n",
    "        # loss = F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "        criterion = nn.CrossEntropyLoss() # sum up batch loss\n",
    "        loss = criterion(output,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test( model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./train/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb4c3af33484c68b9484758f2f5b433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train/MNIST/raw/train-images-idx3-ubyte.gz to ./train/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./train/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f2f72318c94c5896c32a80498828bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train/MNIST/raw/train-labels-idx1-ubyte.gz to ./train/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./train/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31aede2b1b0433ca0db8620c97e15e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train/MNIST/raw/t10k-images-idx3-ubyte.gz to ./train/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./train/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a7227bb80f24acd99aa614828848050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./train/MNIST/raw\n",
      "Processing...\n",
      "Done!\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./test/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f44ea4168c6402288b911fc9aeb29e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./test/MNIST/raw/train-images-idx3-ubyte.gz to ./test/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./test/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567751ffffe1437b9323136de82db0fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./test/MNIST/raw/train-labels-idx1-ubyte.gz to ./test/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./test/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e549fcdb7c044f86a7d764c797275748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./test/MNIST/raw/t10k-images-idx3-ubyte.gz to ./test/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./test/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b77d03bc9f4ae3b42098495e6ee95a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./test/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./test/MNIST/raw\n",
      "Processing...\n",
      "Done!\n",
      "\n",
      "\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.305031\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.298522\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.284686\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.261117\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.963376\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.655745\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.951834\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.599735\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.770311\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.340462\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.298734\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.291193\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.268067\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.288113\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.259382\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.216780\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.151267\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.138235\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.230820\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.170030\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.217107\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.106154\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.156421\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.149568\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.185609\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.199106\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.225110\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.129852\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.079605\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.102056\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.123212\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.114133\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.081466\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.127941\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.134793\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.195196\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.081788\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.103961\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.101970\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.056041\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.047132\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.062467\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.076129\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.126891\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.238595\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.208075\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.073209\n",
      "\n",
      "Test set: Average loss: 0.0849, Accuracy: 9730/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.068374\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.069337\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.074361\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.146984\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.035045\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.134089\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.131702\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.076462\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.182357\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.020076\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.078570\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.044874\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.080614\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.033339\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.102852\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.095274\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.086669\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.088608\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.058900\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.044338\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.053607\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.049952\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.068048\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.084178\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.061609\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.082196\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.060939\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.023973\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.079401\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.085419\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.083941\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.144847\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.072411\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.073455\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.047174\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.073693\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.071779\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.012590\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.101432\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.066277\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.041431\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.131184\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.022917\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.043649\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.044867\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.039714\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.061459\n",
      "\n",
      "Test set: Average loss: 0.0673, Accuracy: 9779/10000 (98%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.004770\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.054016\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.064950\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.083896\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.028746\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.080255\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.044024\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.050267\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.090834\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.040260\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.040650\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6949c9b73d1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Traning and Testing total excution time is: %s seconds '\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtime0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-6949c9b73d1c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-25554dfdf85f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# sum up batch loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    time0 = time.time()\n",
    "    # Training settings\n",
    "    batch_size = 128\n",
    "    epochs = 50\n",
    "    lr = 0.05\n",
    "    no_cuda = True\n",
    "    save_model = False\n",
    "    use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "    torch.manual_seed(100)\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    trainset = torchvision.datasets.MNIST(root='./train', train=True, download=True, transform=ToTensor())\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
    "    testset = torchvision.datasets.MNIST(root='./test', train=False, download=True, transform=ToTensor())\n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False)\n",
    "\n",
    "    model = LeNet().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train( model, device, train_loader, optimizer, epoch)\n",
    "        test( model, device, test_loader)\n",
    "\n",
    "    if (save_model):\n",
    "        torch.save(model.state_dict(),\"cifar_lenet.pt\")\n",
    "    time1 = time.time() \n",
    "    print ('Traning and Testing total excution time is: %s seconds ' % (time1-time0))   \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
